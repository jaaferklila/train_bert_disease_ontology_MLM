{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8996902,"sourceType":"datasetVersion","datasetId":5268933}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Train a Tokonizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T11:50:59.755929Z","iopub.execute_input":"2024-07-06T11:50:59.756187Z","iopub.status.idle":"2024-07-06T11:51:38.814475Z","shell.execute_reply.started":"2024-07-06T11:50:59.756163Z","shell.execute_reply":"2024-07-06T11:51:38.812705Z"}}},{"cell_type":"code","source":"!pip install transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom accelerate import Accelerator\nimport os\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"GPU is available and being used.\")\nelse:\n    device = torch.device('cpu')\n    print(\"GPU is not available, using CPU.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:48:50.035792Z","iopub.execute_input":"2024-07-20T11:48:50.036121Z","iopub.status.idle":"2024-07-20T11:49:07.999015Z","shell.execute_reply.started":"2024-07-20T11:48:50.036094Z","shell.execute_reply":"2024-07-20T11:49:07.998205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-20 11:48:57.011815: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-20 11:48:57.011929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-20 11:48:57.129591: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"#import shutil\n\n# Remove the folder and its contents\n#shutil.rmtree('/kaggle/working/diseaseBert')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# diseaseBert1_60_epochs_with_old_tokenizer","metadata":{}},{"cell_type":"code","source":"\n# Charger le tokenizer et le modèle BERT pour le MLM\nmodel_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)  \n\n# Charger le fichier texte\nadditional_texts = []\nwith open('/kaggle/input/ccccccc/cleaned_concatenated_text_file.txt', 'r', encoding='utf-8') as file:  \n    for line in file:\n        additional_texts.append(line.strip())\n\n# Convertir en Dataset\nadditional_dataset = Dataset.from_dict({\"text\": additional_texts})\n\n# Tokeniser le dataset additionnel\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)  \n\ntokenized_additional_dataset = additional_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Préparer le collateur de données pour MLM\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\n\n\n\ntraining_args = TrainingArguments(\n     output_dir=\"/kaggle/working/diseaseBert_old_token\",\n    overwrite_output_dir=True,\n    num_train_epochs=60, \n    per_device_train_batch_size=8,  \n    save_steps=8000,  # \n    save_total_limit=2,\n    prediction_loss_only=True,\n    learning_rate=5e-5,  \n    weight_decay=0.01,  \n    fp16=True  \n)\n\n# Initialiser le Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_additional_dataset,\n)\n\n# Entraîner le modèle\ntrainer.train()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:50:43.151530Z","iopub.execute_input":"2024-07-20T11:50:43.151968Z","iopub.status.idle":"2024-07-20T16:01:13.129082Z","shell.execute_reply.started":"2024-07-20T11:50:43.151935Z","shell.execute_reply":"2024-07-20T16:01:13.128209Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"GPU is available and being used.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"156e11688a5b40128b04d1907e014b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee046290ed364746b1994988cd95c42f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c05f5b77ca41dca95bcb0c2586f068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"145adbabedd34a9eae83b5967e81ec9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f963c4626ecd4c00a3c82aa1a9dc7706"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11522 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c731a9b5fda4c6d9115192c7ec403a4"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240720_115137-2i9grvhz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/klilajaafer/huggingface/runs/2i9grvhz' target=\"_blank\">/kaggle/working/diseaseBert_old_token</a></strong> to <a href='https://wandb.ai/klilajaafer/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/klilajaafer/huggingface' target=\"_blank\">https://wandb.ai/klilajaafer/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/klilajaafer/huggingface/runs/2i9grvhz' target=\"_blank\">https://wandb.ai/klilajaafer/huggingface/runs/2i9grvhz</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='43260' max='43260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [43260/43260 4:09:17, Epoch 60/60]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.387200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.182500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.096100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.022900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.976000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.938700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.906000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.873700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.854000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.826200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.811600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.797800</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.774600</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.743300</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.733900</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.698700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.691400</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.696400</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.690400</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.648800</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.647400</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.632800</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.615900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.610700</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.625100</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.585900</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.579500</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.563200</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.567300</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.552200</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.526800</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.524100</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.513300</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.509500</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.499300</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.504400</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.476800</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.472200</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.487400</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.453700</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.450500</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.457800</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.448900</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.440900</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.435200</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.423400</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.413800</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.419000</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.397800</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.403900</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.392300</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.387900</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.382000</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.380800</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.385800</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.375300</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.366100</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.350400</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.367900</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.367800</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.338100</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.340400</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.326000</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.323700</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>0.315400</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>0.322400</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>0.319900</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>0.325500</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>0.310400</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>0.296900</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>0.306400</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>0.296900</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>0.301500</td>\n    </tr>\n    <tr>\n      <td>38000</td>\n      <td>0.293500</td>\n    </tr>\n    <tr>\n      <td>38500</td>\n      <td>0.290800</td>\n    </tr>\n    <tr>\n      <td>39000</td>\n      <td>0.289800</td>\n    </tr>\n    <tr>\n      <td>39500</td>\n      <td>0.286500</td>\n    </tr>\n    <tr>\n      <td>40000</td>\n      <td>0.297500</td>\n    </tr>\n    <tr>\n      <td>40500</td>\n      <td>0.281700</td>\n    </tr>\n    <tr>\n      <td>41000</td>\n      <td>0.291400</td>\n    </tr>\n    <tr>\n      <td>41500</td>\n      <td>0.285500</td>\n    </tr>\n    <tr>\n      <td>42000</td>\n      <td>0.281900</td>\n    </tr>\n    <tr>\n      <td>42500</td>\n      <td>0.279100</td>\n    </tr>\n    <tr>\n      <td>43000</td>\n      <td>0.273900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=43260, training_loss=0.5138764556447389, metrics={'train_runtime': 15021.5046, 'train_samples_per_second': 46.022, 'train_steps_per_second': 2.88, 'total_flos': 4.5489687796224e+16, 'train_loss': 0.5138764556447389, 'epoch': 60.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_hub\nfrom huggingface_hub import notebook_login\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T16:01:55.379245Z","iopub.execute_input":"2024-07-20T16:01:55.379907Z","iopub.status.idle":"2024-07-20T16:02:07.855750Z","shell.execute_reply.started":"2024-07-20T16:01:55.379872Z","shell.execute_reply":"2024-07-20T16:02:07.854520Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T16:03:35.794325Z","iopub.execute_input":"2024-07-20T16:03:35.795000Z","iopub.status.idle":"2024-07-20T16:03:35.818874Z","shell.execute_reply.started":"2024-07-20T16:03:35.794967Z","shell.execute_reply":"2024-07-20T16:03:35.818226Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a80a1c419d491abae3099780bd357d"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T16:05:36.403628Z","iopub.execute_input":"2024-07-20T16:05:36.403989Z","iopub.status.idle":"2024-07-20T16:05:48.698830Z","shell.execute_reply.started":"2024-07-20T16:05:36.403960Z","shell.execute_reply":"2024-07-20T16:05:48.697926Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1721476251.a2ca2d850ec6.34.0:   0%|          | 0.00/23.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c497f2b9598406c95bbb0c01383d796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7828c6c17e534d29bbb4b38167684c8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d181fcc8a0b44c20bcca45e4159afa0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5fd7bddbfa4c158f372af363434730"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Jaafer/diseaseBert_old_token/commit/d4747135f292ea7cf838062cfa0ec75717560187', commit_message='End of training', commit_description='', oid='d4747135f292ea7cf838062cfa0ec75717560187', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}